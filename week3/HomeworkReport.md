
## Methodology

We tested the application with various language models to evaluate their performance across several dimensions:

1. Character retention: How well each model remembered and utilized character details
2. Context window utilization: Effects of different context window sizes on story coherence
3. Model size impact: Differences in output quality and character consistency based on model parameters

### Key Findings

#### Character Memory

- Larger models maintained character consistency throughout longer stories
- Medium models performed well with up to 3-4 characters but showed degradation with more
- Smaller models struggled with maintaining consistent character attributes beyond basic names and simple traits

#### Context Window Impact

- Larger context windows (8K-16K) allowed for more detailed character descriptions and better retention
- Medium context windows (4K) worked well for stories with 2-3 characters with moderate detail
- Smaller context windows (2K) showed character detail reduction


#### Model Size vs. Performance

- Larger models produced more nuanced character interactions and better narrative arcs
- Smaller models struggled to maintain character consistency as the number of characters was increasing
- The quality difference was a little less visible for simple stories with 1-2 characters